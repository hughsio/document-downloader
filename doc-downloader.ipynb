{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mESPZyJu8-LV"
      },
      "source": [
        "# Overview\n",
        "This script allows you to scrape a website to download documents. Please use this with caution, be sure to have permission to do this before you do. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r47i7I0UuB77"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import ssl\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "\n",
        "# Set the url for the website here\n",
        "\n",
        "website_url = 'https://example.com'\n",
        "\n",
        "# Set the output directory for downloaded files\n",
        "output_directory = 'files'\n",
        "download_timeout = 6  # Timeout in seconds\n",
        "max_file_size = 120 * 1024 * 1024  # Maximum file size to download in bytes (120MB in this case)\n",
        "\n",
        "# Create a output directory if one is already created.\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Important\n",
        "You may need to create custom SSL certificates but I'm not sure how to do that yet but will figure it out and update this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjwHR_z-Hnqs"
      },
      "outputs": [],
      "source": [
        "# Create a custom SSL context\n",
        "context = ssl.create_default_context()\n",
        "context.check_hostname = False\n",
        "context.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "# Function to download a file\n",
        "def download_file(file_url, file_name, max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            with requests.get(file_url, stream=True, verify=False, timeout=download_timeout) as r:\n",
        "                r.raise_for_status()\n",
        "                if int(r.headers.get('content-length', 0)) > max_file_size:\n",
        "                    print(f\"File {file_name} is too large. Skipping...\")\n",
        "                    return False\n",
        "                with open(file_name, 'wb') as f:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "            print(f'Successfully downloaded: {file_name}')\n",
        "            return True\n",
        "        except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:\n",
        "            print(f\"Attempt {attempt + 1} failed for {file_url}: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"Retrying in 2 seconds...\")\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print(f\"Max retries reached. Moving to next file.\")\n",
        "    return False\n",
        "\n",
        "response = requests.get(website_url, verify=False)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "for link in soup.find_all('a', href=True):\n",
        "    file_url = link['href']\n",
        "    # Convert relative URL to absolute URL\n",
        "    file_url = urljoin(website_url, file_url)\n",
        "    if file_url.endswith(('.pdf', '.docx', '.xlsx')):  # Add other document types as needed\n",
        "        file_name = os.path.join(output_directory, os.path.basename(file_url))\n",
        "        download_file(file_url, file_name)\n",
        "\n",
        "print(\"Download process completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Part\n",
        "Now, to download the file. I used Shutil to make a zip file. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P9pelqDJ7Jf",
        "outputId": "bb8ad192-cb18-4dd8-f3a2-4fcdf0a753e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zip file created!\n"
          ]
        }
      ],
      "source": [
        "# create a zip file from Publications\n",
        "\n",
        "import shutil\n",
        "shutil.make_archive('/content/folder', 'zip', '/content/folder')\n",
        "print(\"Zip file created!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
